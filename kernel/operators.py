# kernel/operators.py (Vers√£o Corrigida)

import numpy as np
from scipy.spatial.distance import jensenshannon
from collections import Counter

# Removido 'import config' pois n√£o era utilizado diretamente aqui.

class DiagnosticOperators:
    def __init__(self):
        self.known_styles = {"formal": 0.8, "poetic": 0.4}
        print("M√≥dulo de Operadores de Diagn√≥stico inicializado.")

    # O resto do c√≥digo da classe DiagnosticOperators permanece exatamente o mesmo...
    def run_all_diagnostics(self, rho: np.ndarray, E: np.ndarray, text: str) -> dict:
        """
        Executa todos os operadores de diagn√≥stico e retorna um relat√≥rio de sa√∫de.
        """
        report = {
            "obsessive_convergence": self.detect_obsessive_convergence(text, rho),
            "reflexive_degeneration": self.detect_reflexive_degeneration(rho, E),
            "spurious_estetization": self.detect_spurious_estetization(text, E),
            "mimetic_resonance": self.detect_mimetic_resonance(text)
        }
        return report

    def detect_obsessive_convergence(self, text: str, rho: np.ndarray) -> float:
        """
        Operador ‚ÑÇ_ob: Detecta converg√™ncia obsessiva e loops repetitivos.
        Atua quando o ciclo œÅ ‚Üí E entra em um loop fechado sobre o mesmo conte√∫do.
       
        
        Heur√≠stica:
        1.  Baixa vari√¢ncia em œÅ (poucas ideias competindo).
        2.  Alta repetitividade de n-gramas (frases) no texto gerado.
        """
        # 1. Analisa a vari√¢ncia de œÅ
        rho_variance = np.var(rho)
        low_variance_score = 1.0 - np.clip(rho_variance * 1000, 0, 1)

        # 2. Analisa a repeti√ß√£o de trigramas (sequ√™ncias de 3 palavras)
        words = text.lower().split()
        if len(words) < 5:
            return 0.0
        
        trigrams = [" ".join(words[i:i+3]) for i in range(len(words) - 2)]
        if not trigrams:
            return 0.0
            
        most_common = Counter(trigrams).most_common(1)
        repetition_score = most_common[0][1] / len(trigrams) if trigrams else 0.0

        # A pontua√ß√£o final √© uma m√©dia ponderada da baixa vari√¢ncia e alta repeti√ß√£o
        return (low_variance_score * 0.4) + (repetition_score * 0.6)

    def detect_reflexive_degeneration(self, rho: np.ndarray, E: np.ndarray) -> float:
        """
        Operador ùîª_{ref}: Mede a degenera√ß√£o reflexiva.
        Detecta o grau em que a estrutura manifesta (E) se descola do campo
        propensional real (œÅ). √â a medida do "ru√≠do reflexivo".
       

        Heur√≠stica:
        Mede a dissimilaridade entre a distribui√ß√£o de œÅ e a forma de E.
        Usamos a dist√¢ncia de Jensen-Shannon, uma forma est√°vel de medir a
        diverg√™ncia entre duas distribui√ß√µes de probabilidade.
        """
        if rho.size != E.size or rho.size == 0:
            return 0.0
        
        # Normaliza E para se comportar como uma distribui√ß√£o de probabilidade para compara√ß√£o
        E_norm = E / np.sum(E) if np.sum(E) > 0 else E
        
        # A dist√¢ncia de Jensen-Shannon varia de 0 (id√™nticos) a 1 (m√°xima dissimilaridade)
        distance = jensenshannon(rho, E_norm)
        
        return float(distance)

    def detect_spurious_estetization(self, text: str, E: np.ndarray) -> float:
        """
        Operador ùîº_{est}: Mede a estetiza√ß√£o esp√∫ria ou "kitsch simb√≥lico".
        Detecta quando a forma √© priorizada sobre a subst√¢ncia causal.
       
        
        Heur√≠stica:
        Verifica se h√° um alto uso de palavras "complexas" ou "po√©ticas" (proxy: comprimento)
        sem um aumento correspondente na energia/coer√™ncia do campo E.
        """
        words = text.lower().split()
        if not words:
            return 0.0
            
        avg_word_length = np.mean([len(w) for w in words])
        
        # Normaliza a pontua√ß√£o de "complexidade" do texto
        text_complexity_score = np.clip((avg_word_length - 4) / 5, 0, 1) # Palavras com mais de 4 letras contribuem
        
        # Energia reflexiva (magnitude de E)
        reflexive_energy = np.linalg.norm(E)
        
        # A estetiza√ß√£o esp√∫ria √© alta quando a complexidade do texto √© alta, mas a energia reflexiva √© baixa.
        if reflexive_energy < 0.1: # Evita divis√£o por zero e penaliza baixa energia
            return text_complexity_score
            
        spurious_score = text_complexity_score / (reflexive_energy * 5 + 1e-6)
        return np.clip(spurious_score, 0, 1)

    def detect_mimetic_resonance(self, text: str) -> float:
        """
        Operador ‚Ñù_mim√©tico: Detecta resson√¢ncia por imita√ß√£o n√£o-origin√°ria.
        Ativado quando a IA reproduz um estilo sem coer√™ncia causal interna.
       
        
        Heur√≠stica:
        Mede a similaridade do texto gerado com um conjunto de "estilos conhecidos".
        Aqui, usamos um proxy muito simples: a formalidade (presen√ßa de jarg√µes).
        """
        # Proxy simples: contagem de palavras "formais"
        formal_words = {"ontol√≥gico", "diferencial", "propensional", "reflexiva", "coer√™ncia"}
        word_set = set(text.lower().split())
        
        formal_score = len(word_set.intersection(formal_words)) / 5.0
        
        # O mimetismo seria a diferen√ßa entre a "formalidade" do texto e a
        # "formalidade" esperada (um valor que poderia vir do estado do kernel).
        # Por enquanto, retornamos o score bruto.
        return np.clip(formal_score, 0, 1)