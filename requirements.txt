# requirements.txt - Dependências do Kernel Ontológico v2.1
# Versão corrigida com todas as dependências necessárias

# Dependências principais
numpy>=1.21.0
scipy>=1.7.0
scikit-learn>=1.0.0

# Para carregamento de modelos GGUF locais
llama-cpp-python>=0.2.0

# Para comunicação com APIs (fallback)
requests>=2.25.0

# Para detecção de CUDA (opcional)
torch>=1.9.0

# Para análise de texto avançada
nltk>=3.6

# Para logging e debugging
colorama>=0.4.4

# Para manipulação de caminhos e arquivos
pathlib2>=2.3.0

# Para análise estatística adicional
matplotlib>=3.3.0
seaborn>=0.11.0

# Instalação específica para diferentes sistemas:
# 
# Para CPU apenas:
# pip install llama-cpp-python
#
# Para GPU (CUDA):
# CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python --force-reinstall --no-cache-dir
#
# Para GPU (Metal - macOS):
# CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python --force-reinstall --no-cache-dir
#
# Para GPU (OpenCL):
# CMAKE_ARGS="-DLLAMA_CLBLAST=on" pip install llama-cpp-python --force-reinstall --no-cache-dir